from fastapi import APIRouter, HTTPException
from app.config import supabase
from app.services.note_to_task import generate_tasks_from_note
from app.services.task_to_notification import generate_notifications_from_tasks_batch
from app.services.ai_task_executor import execute_ai_task
from app.services.ai_task_executor.completion_notification_service import generate_completion_notification
from app.infra.supabase.repositories.workspaces import WorkspaceRepository, WorkspaceMemberRepository
from app.infra.supabase.repositories.tasks import TaskRepository
from app.infra.supabase.repositories.notifications import AINotificationRepository
from app.infra.supabase.repositories.task_results import TaskResultRepository
from app.models.task import TaskUpdate, Task
from app.models.notification import AINotificationCreate
from app.models.task_result import TaskResultCreate
from app.utils.recurrence_calculator import calculate_next_run_time
import asyncio
from typing import List, Optional, Tuple
from datetime import datetime, timezone, timedelta
import logging

router = APIRouter(prefix="/api/webhooks", tags=["webhooks"])

# é–‹ç™ºè€…ã®user_idï¼ˆæœ¬ç•ªãƒ»localä¸¡æ–¹ã§åŒã˜ï¼‰
DEV_USER_IDS = [
    "58e744e7-ec0f-45e1-a63a-bc6ed71e10de",
]


async def _process_notes_sync(
    minutes_ago: int,
    user_id_filter: Optional[List[str]] = None,
    exclude_user_ids: bool = False
) -> dict:
    """
    DEPRECATED: æ—§ãƒãƒ¼ãƒˆåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚ç¾åœ¨æœªä½¿ç”¨ã€‚
    Memory Serviceï¼ˆ/api/memory/sync-eventsï¼‰ã«ç½®ãæ›ãˆæ¸ˆã¿ã€‚
    å‰Šé™¤äºˆå®šã€‚æ–°è¦åˆ©ç”¨ç¦æ­¢ã€‚

    æ—§å‡¦ç†: ãƒãƒ¼ãƒˆ â†’ generate_tasks_from_note â†’ generate_notifications_from_tasks_batch
    æ–°å‡¦ç†: ãƒãƒ¼ãƒˆ â†’ SourceDiff â†’ MemoryService.process_events (4ã‚¹ãƒ†ãƒƒãƒ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
    """
    from datetime import datetime, timedelta, timezone
    from app.infra.supabase.repositories.notes import NoteRepository
    import logging
    
    logger = logging.getLogger(__name__)
    
    filter_desc = ""
    if user_id_filter:
        if exclude_user_ids:
            filter_desc = " (excluding dev users)"
        else:
            filter_desc = " (dev users only)"
    
    logger.info(f"ğŸ”„ CRON: Starting sync-memories{filter_desc}")
    
    # æŒ‡å®šæ™‚é–“å‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    time_ago = datetime.now(timezone.utc) - timedelta(minutes=minutes_ago)
    
    note_repo = NoteRepository(supabase)
    
    # æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒˆã®ã¿å–å¾—ï¼ˆuser_idãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ï¼‰
    updated_notes = await note_repo.find_updated_since(
        time_ago, 
        user_id_filter=user_id_filter,
        exclude_user_ids=exclude_user_ids
    )
    
    logger.info(f"Found {len(updated_notes)} updated notes{filter_desc}")
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    total_tasks_generated = 0
    total_notifications_generated = 0
    
    # ãƒãƒ¼ãƒˆå‡¦ç†é–¢æ•°
    async def process_note_with_limit(note):
        nonlocal total_tasks_generated, total_notifications_generated
        
        async with semaphore:
            try:
                # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                workspace_repo = WorkspaceRepository(supabase)
                workspace = await workspace_repo.find_by_id(note.workspace_id)
                
                if not workspace:
                    return {"status": "error", "note_id": note.id, "reason": "workspace_not_found"}
                
                if not note.text:
                    return {"status": "skipped", "note_id": note.id, "reason": "empty_text"}
                
                # workspace typeã«å¿œã˜ã¦(user_id, workspace_member_id)ãƒšã‚¢ã‚’å–å¾—
                user_workspace_member_pairs: List[Tuple[str, Optional[int]]] = []
                
                if workspace.type == "personal":
                    # personal workspaceã®å ´åˆ: ã‚ªãƒ¼ãƒŠãƒ¼ã®user_idã¨workspace_member_idã‚’å–å¾—
                    workspace_member_repo = WorkspaceMemberRepository(supabase)
                    members = await workspace_member_repo.find_by_workspace(note.workspace_id)
                    
                    if not members:
                        return {"status": "error", "note_id": note.id, "reason": "no_workspace_members"}
                    
                    user_workspace_member_pairs = [(members[0].user_id, members[0].id)]
                    
                elif workspace.type == "group":
                    # group workspaceã®å ´åˆ: assigneeã®(user_id, workspace_member_id)ãƒšã‚¢ã‚’å–å¾—
                    pairs = await note_repo.get_note_assignee_user_and_member_ids(note.id)
                    user_workspace_member_pairs = [(user_id, workspace_member_id) for user_id, workspace_member_id in pairs]
                    
                    if not user_workspace_member_pairs:
                        return {"status": "skipped", "note_id": note.id, "reason": "no_assignees"}
                
                # ãƒãƒ¼ãƒˆâ†’ã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼ˆ1å›ã®LLMå‘¼ã³å‡ºã—ã§å…¨ãƒšã‚¢åˆ†ã®ã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼‰
                tasks = await generate_tasks_from_note(note.id, note.text, user_workspace_member_pairs, note.title)
                tasks_count = len(tasks)
                total_tasks_generated += tasks_count
                
                # ã‚¿ã‚¹ã‚¯ãŒç”Ÿæˆã•ã‚ŒãŸã‚‰ã€å³åº§ã«é€šçŸ¥ã‚’ç”Ÿæˆ
                notifications_count = 0
                if tasks:
                    notifications = await generate_notifications_from_tasks_batch(tasks)
                    notifications_count = len(notifications)
                    total_notifications_generated += notifications_count
                    
                    logger.info(f"âœ… Note {note.id}: Generated {tasks_count} tasks and {notifications_count} notifications")
                else:
                    logger.info(f"âœ… Note {note.id}: No tasks generated")
                
                return {
                    "status": "ok",
                    "note_id": note.id,
                    "tasks_count": tasks_count,
                    "notifications_count": notifications_count
                }
                
            except Exception as e:
                logger.error(f"âŒ Error processing note {note.id}: {e}")
                return {"status": "error", "note_id": note.id, "error": str(e)}
    
    # ãƒãƒ¼ãƒˆã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_note_with_limit(note) for note in updated_notes],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(updated_notes)} notes processed{filter_desc}")
    logger.info(f"ğŸ“Š Generated {total_tasks_generated} tasks and ~{total_notifications_generated} notifications")
    
    return {
        "status": "ok",
        "notes_processed": success,
        "notes_total": len(updated_notes),
        "tasks_generated": total_tasks_generated,
        "notifications_generated": total_notifications_generated
    }


@router.post("/sync-memories")
async def sync_memories():
    """
    DEPRECATED: æ—§æœ¬ç•ªç”¨CRONã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚ç¾åœ¨æœªä½¿ç”¨ã€‚
    ä»£æ›¿: /api/memory/sync-eventsï¼ˆMemory Serviceãƒ™ãƒ¼ã‚¹ã®æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰
    """
    return await _process_notes_sync(
        minutes_ago=5,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=True
    )


@router.post("/sync-memories-local")
async def sync_memories_local():
    """
    DEPRECATED: æ—§ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨CRONã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚ç¾åœ¨æœªä½¿ç”¨ã€‚
    ä»£æ›¿: /api/memory/sync-eventsï¼ˆMemory Serviceãƒ™ãƒ¼ã‚¹ã®æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰
    """
    return await _process_notes_sync(
        minutes_ago=1,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=False
    )


@router.post("/process-recurring-tasks")
async def process_recurring_tasks():
    """
    å®šæœŸã‚¿ã‚¹ã‚¯ã®æ¬¡å›å®Ÿè¡Œæ™‚åˆ»ã‚’æ›´æ–°ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ1æ—¥1å›ã€åˆå‰0æ™‚ã«å®Ÿè¡Œï¼‰
    
    å‡¦ç†å†…å®¹:
    1. æ˜¨æ—¥ã®åˆå‰0æ™‚ã€œä»Šæ—¥ã®åˆå‰0æ™‚ã®ç¯„å›²ã§next_run_timeãŒè©²å½“ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    2. å„ã‚¿ã‚¹ã‚¯ã®next_run_timeã‚’æ¬¡å›å®Ÿè¡Œæ™‚åˆ»ã«æ›´æ–°
    3. is_recurring_task_active=Trueã®ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€æ—¢å­˜ã®é€šçŸ¥ã‚’è¤‡è£½ã—ã¦æ¬¡å›åˆ†ã‚’ä½œæˆ
    """
    logger = logging.getLogger(__name__)
    logger.info("ğŸ”„ CRON: Starting process-recurring-tasks")
    
    # æ˜¨æ—¥ã®åˆå‰0æ™‚ã¨ä»Šæ—¥ã®åˆå‰0æ™‚ã‚’å–å¾—
    now = datetime.now(timezone.utc)
    today_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)
    yesterday_midnight = today_midnight - timedelta(days=1)
    
    logger.info(f"Processing tasks with next_run_time between {yesterday_midnight} and {today_midnight}")
    
    # TaskRepositoryã§ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    task_repo = TaskRepository(supabase)
    notification_repo = AINotificationRepository(supabase)
    
    # next_run_timeãŒæ˜¨æ—¥ã®åˆå‰0æ™‚ã€œä»Šæ—¥ã®åˆå‰0æ™‚ã®ç¯„å›²ã«ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    # recurrence_patternãŒNULLã§ãªã„ã‚‚ã®ãŒå¯¾è±¡
    response = (
        supabase.table("tasks")
        .select("*")
        .gte("next_run_time", yesterday_midnight.isoformat())
        .lt("next_run_time", today_midnight.isoformat())
        .not_.is_("recurrence_pattern", "null")
        .execute()
    )
    
    recurring_tasks = response.data if response.data else []
    logger.info(f"Found {len(recurring_tasks)} recurring tasks to process")
    
    if not recurring_tasks:
        return {
            "status": "ok",
            "tasks_processed": 0,
            "notifications_created": 0
        }
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    tasks_updated = 0
    tasks_stopped = 0
    notifications_created = 0
    
    # ã‚¿ã‚¹ã‚¯å‡¦ç†é–¢æ•°
    async def process_task_with_limit(task_data):
        nonlocal tasks_updated, tasks_stopped, notifications_created
        
        async with semaphore:
            try:
                task_id = task_data["id"]
                old_next_run_time = datetime.fromisoformat(task_data["next_run_time"].replace("Z", "+00:00"))
                recurrence_pattern = task_data["recurrence_pattern"]
                is_active = task_data.get("is_recurring_task_active", True)
                is_ai_task = task_data.get("is_ai_task", False)
                
                # is_ai_taskã®å ´åˆã€æœªè§£æ±ºã®é€šçŸ¥ãŒ3å€‹ä»¥ä¸Šã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if is_ai_task and is_active:
                    # statusãŒ"resolved"ã§ãªã„é€šçŸ¥ã‚’å–å¾—
                    response = (
                        supabase.table("ai_notifications")
                        .select("*")
                        .eq("task_id", task_id)
                        .neq("status", "resolved")
                        .execute()
                    )
                    unresolved_notifications = response.data if response.data else []
                    unresolved_count = len(unresolved_notifications)
                    
                    if unresolved_count >= 3:
                        # æœªè§£æ±ºé€šçŸ¥ãŒ3å€‹ä»¥ä¸Šã‚ã‚‹å ´åˆã€ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢
                        update_data = TaskUpdate(is_recurring_task_active=False)
                        await task_repo.update(task_id, update_data)
                        tasks_stopped += 1
                        
                        logger.warning(
                            f"âš ï¸ Task {task_id}: Stopped due to {unresolved_count} unresolved notifications. "
                            f"is_recurring_task_active set to False."
                        )
                        
                        return {
                            "status": "stopped",
                            "task_id": task_id,
                            "reason": f"unresolved_notifications >= 3 ({unresolved_count})",
                            "notifications_created": 0
                        }
                
                # æ–°ã—ã„next_run_timeã‚’è¨ˆç®—
                new_next_run_time = calculate_next_run_time(old_next_run_time, recurrence_pattern)
                
                # ã‚¿ã‚¹ã‚¯ã®next_run_timeã‚’æ›´æ–°
                update_data = TaskUpdate(next_run_time=new_next_run_time)
                await task_repo.update(task_id, update_data)
                tasks_updated += 1
                
                logger.info(f"âœ… Task {task_id}: Updated next_run_time from {old_next_run_time} to {new_next_run_time}")
                
                # is_recurring_task_active=Trueã®å ´åˆã®ã¿ã€é€šçŸ¥ã‚’è¤‡è£½
                existing_notifications = []
                if is_active:
                    # æ—¢å­˜ã®é€šçŸ¥ã‚’å–å¾—
                    existing_notifications = await notification_repo.find_by_filters({"task_id": task_id})
                    
                    if existing_notifications:
                        # å„é€šçŸ¥ã‚’è¤‡è£½
                        for notification in existing_notifications:
                            # å…ƒã®due_dateã¨old_next_run_timeã®å·®åˆ†ã‚’è¨ˆç®—
                            time_diff = notification.due_date - old_next_run_time
                            
                            # æ–°ã—ã„due_dateã‚’è¨ˆç®—
                            new_due_date = new_next_run_time + time_diff
                            
                            # æ–°ã—ã„é€šçŸ¥ã‚’ä½œæˆ
                            new_notification = AINotificationCreate(
                                title=notification.title,
                                body=notification.body,
                                due_date=new_due_date,
                                task_id=task_id,
                                workspace_id=notification.workspace_id,
                                workspace_member_id=notification.workspace_member_id,
                                status=notification.status
                            )
                            
                            await notification_repo.create(new_notification)
                            notifications_created += 1
                            
                            logger.info(f"ğŸ“¬ Task {task_id}: Created notification with due_date {new_due_date}")
                    else:
                        logger.info(f"â„¹ï¸ Task {task_id}: No existing notifications to duplicate")
                else:
                    logger.info(f"â„¹ï¸ Task {task_id}: is_recurring_task_active=False, skipping notification duplication")
                
                return {
                    "status": "ok",
                    "task_id": task_id,
                    "notifications_created": len(existing_notifications) if is_active else 0
                }
                
            except Exception as e:
                logger.error(f"âŒ Error processing task {task_data.get('id')}: {e}")
                return {"status": "error", "task_id": task_data.get("id"), "error": str(e)}
    
    # ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_task_with_limit(task) for task in recurring_tasks],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(recurring_tasks)} tasks processed")
    logger.info(f"ğŸ“Š Tasks updated: {tasks_updated}, Tasks stopped: {tasks_stopped}, Notifications created: {notifications_created}")
    
    return {
        "status": "ok",
        "tasks_processed": tasks_updated,
        "tasks_stopped": tasks_stopped,
        "notifications_created": notifications_created
    }


async def _execute_ai_tasks_common(
    minutes_ahead: int,
    user_id_filter: Optional[List[str]] = None,
    exclude_user_ids: bool = False
) -> dict:
    """
    AIã‚¿ã‚¹ã‚¯è‡ªå‹•å®Ÿè¡Œã®å…±é€šå‡¦ç†
    
    Args:
        minutes_ahead: ä½•åˆ†å…ˆã¾ã§ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‹
        user_id_filter: æŒ‡å®šã•ã‚ŒãŸuser_idã®ã‚¿ã‚¹ã‚¯ã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        exclude_user_ids: Trueã®å ´åˆã€user_id_filterã«å«ã¾ã‚Œã‚‹user_idã‚’é™¤å¤–
    
    Returns:
        å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    logger = logging.getLogger(__name__)
    
    filter_desc = ""
    if user_id_filter:
        if exclude_user_ids:
            filter_desc = " (excluding dev users)"
        else:
            filter_desc = " (dev users only)"
    
    logger.info(f"ğŸ”„ CRON: Starting execute-ai-tasks{filter_desc}")
    
    # ç¾åœ¨æ™‚åˆ»ã¨æŒ‡å®šåˆ†å¾Œã®æ™‚åˆ»ã‚’å–å¾—
    now = datetime.now(timezone.utc)
    target_time = now + timedelta(minutes=minutes_ahead)
    
    logger.info(f"Processing AI tasks with next_run_time between {now} and {target_time}{filter_desc}")
    
    # TaskResultRepositoryã¨AINotificationRepositoryã‚’åˆæœŸåŒ–
    task_result_repo = TaskResultRepository(supabase)
    notification_repo = AINotificationRepository(supabase)
    
    # æ¡ä»¶ã«åˆã†ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    # is_recurring_task_active=True AND is_ai_task=True AND
    # now < next_run_time <= target_time
    try:
        response = (
            supabase.table("tasks")
            .select("*")
            .eq("is_recurring_task_active", True)
            .eq("is_ai_task", True)
            .gt("next_run_time", now.isoformat())
            .lte("next_run_time", target_time.isoformat())
            .execute()
        )
        
        ai_tasks = response.data if response.data else []
        logger.info(f"Found {len(ai_tasks)} AI tasks to execute{filter_desc}")
        
    except Exception as e:
        logger.error(f"Failed to query AI tasks: {e}")
        return {
            "status": "error",
            "error": str(e),
            "tasks_executed": 0
        }
    
    # user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨ï¼ˆworkspace_member_noteçµŒç”±ã§assigneeã®user_idã‚’å–å¾—ï¼‰
    if user_id_filter and ai_tasks:
        from app.infra.supabase.repositories.notes import NoteRepository
        note_repo = NoteRepository(supabase)
        filtered_tasks = []
        for task in ai_tasks:
            note_id = task.get("source_note_id")
            if not note_id:
                if exclude_user_ids:
                    filtered_tasks.append(task)  # noteç„¡ã—ã¯devé™¤å¤–æ™‚ã¯å«ã‚ã‚‹
                continue
            assignee_user_ids = await note_repo.get_note_assignee_user_ids(note_id)
            has_match = any(uid in user_id_filter for uid in assignee_user_ids)
            if exclude_user_ids and not has_match:
                filtered_tasks.append(task)
            elif not exclude_user_ids and has_match:
                filtered_tasks.append(task)
        ai_tasks = filtered_tasks
        logger.info(f"After user_id filtering: {len(ai_tasks)} AI tasks")
    
    if not ai_tasks:
        return {
            "status": "ok",
            "tasks_executed": 0,
            "results_saved": 0
        }
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    tasks_executed = 0
    results_saved = 0
    notifications_created = 0
    
    # ã‚¿ã‚¹ã‚¯å‡¦ç†é–¢æ•°
    async def process_ai_task_with_limit(task_data):
        nonlocal tasks_executed, results_saved, notifications_created
        
        async with semaphore:
            try:
                # Taskãƒ‡ãƒ¼ã‚¿ã‚’Taskãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
                task = Task(**task_data)
                
                logger.info(f"ğŸ¤– Executing AI task {task.id}: {task.title}")
                
                # AIã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆtitleã¨textã®ä¸¡æ–¹ã‚’å–å¾—ï¼‰
                result_title, result_text = await execute_ai_task(task)
                tasks_executed += 1
                
                # çµæœã‚’task_resultsã«ä¿å­˜
                task_result_create = TaskResultCreate(
                    task_id=task.id,
                    result_title=result_title,
                    result_text=result_text,
                    executed_at=datetime.now(timezone.utc)
                )
                
                saved_result = await task_result_repo.create(task_result_create)
                results_saved += 1
                
                logger.info(f"âœ… Task {task.id} executed and saved: result_id={saved_result.id}")
                
                # å®Œäº†é€šçŸ¥ã‚’ç”Ÿæˆ
                # due_dateã¯next_run_timeã‚’ä½¿ç”¨ï¼ˆAIã‚¿ã‚¹ã‚¯ã§ã¯å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
                if task.next_run_time:
                    due_date = task.next_run_time
                    
                    try:
                        notification_creates = await generate_completion_notification(
                            task=task,
                            result_title=result_title,
                            result_text=result_text,
                            due_date=due_date,
                        )

                        # é€šçŸ¥ã‚’ä¿å­˜ï¼ˆassigneeã”ã¨ï¼‰
                        for notif_create in notification_creates:
                            saved_notification = await notification_repo.create(notif_create)
                            notifications_created += 1
                            logger.info(f"ğŸ“¬ Task {task.id}: Created completion notification {saved_notification.id} with due_date {due_date}")
                    except Exception as e:
                        logger.error(f"Failed to create completion notification for task {task.id}: {e}")
                else:
                    logger.warning(f"Task {task.id}: next_run_time is None, skipping notification creation")

                return {
                    "status": "ok",
                    "task_id": task.id,
                    "result_id": saved_result.id
                }
                
            except Exception as e:
                logger.error(f"âŒ Error executing AI task {task_data.get('id')}: {e}")
                return {"status": "error", "task_id": task_data.get("id"), "error": str(e)}
    
    # ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_ai_task_with_limit(task) for task in ai_tasks],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(ai_tasks)} AI tasks executed{filter_desc}")
    logger.info(f"ğŸ“Š Tasks executed: {tasks_executed}, Results saved: {results_saved}, Notifications created: {notifications_created}")
    
    return {
        "status": "ok",
        "tasks_executed": tasks_executed,
        "results_saved": results_saved,
        "notifications_created": notifications_created
    }


@router.post("/execute-ai-tasks")
async def execute_ai_tasks():
    """
    æœ¬ç•ªç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ10åˆ†ã”ã¨ï¼‰
    - æ¬¡ã®10åˆ†ä»¥å†…ã«next_run_timeãŒæ¥ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    - é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
    """
    return await _execute_ai_tasks_common(
        minutes_ahead=10,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=True
    )


@router.post("/execute-ai-tasks-local")
async def execute_ai_tasks_local():
    """
    ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ1åˆ†ã”ã¨ï¼‰
    - æ¬¡ã®1åˆ†ä»¥å†…ã«next_run_timeãŒæ¥ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    - é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’å‡¦ç†
    """
    return await _execute_ai_tasks_common(
        minutes_ahead=1,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=False
    )


@router.post("/execute-ai-task/{task_id}")
async def execute_ai_task_by_id(task_id: int):
    """
    ç‰¹å®šã®task_idã§AIã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Args:
        task_id: å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ID
    
    Returns:
        å®Ÿè¡Œçµæœã®çµ±è¨ˆæƒ…å ±
    """
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ”„ Executing AI task by ID: {task_id}")
    
    # TaskRepositoryã¨TaskResultRepositoryã€AINotificationRepositoryã‚’åˆæœŸåŒ–
    task_repo = TaskRepository(supabase)
    task_result_repo = TaskResultRepository(supabase)
    notification_repo = AINotificationRepository(supabase)
    
    # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    task = await task_repo.find_by_id(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
    
    # AIã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
    if not task.is_ai_task:
        raise HTTPException(
            status_code=400, 
            detail=f"Task {task_id} is not an AI task (is_ai_task=False)"
        )
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
    if not task.is_recurring_task_active:
        raise HTTPException(
            status_code=400,
            detail=f"Task {task_id} is not active (is_recurring_task_active=False)"
        )
    
    try:
        logger.info(f"ğŸ¤– Executing AI task {task.id}: {task.title}")
        
        # AIã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        result_title, result_text = await execute_ai_task(task)
        
        # çµæœã‚’task_resultsã«ä¿å­˜
        task_result_create = TaskResultCreate(
            task_id=task.id,
            result_title=result_title,
            result_text=result_text,
            executed_at=datetime.now(timezone.utc)
        )
        
        saved_result = await task_result_repo.create(task_result_create)
        logger.info(f"âœ… Task {task.id} executed and saved: result_id={saved_result.id}")
        
        # å®Œäº†é€šçŸ¥ã‚’ç”Ÿæˆ
        notification_created = False
        if task.next_run_time:
            due_date = task.next_run_time
            
            try:
                notification_creates = await generate_completion_notification(
                    task=task,
                    result_title=result_title,
                    result_text=result_text,
                    due_date=due_date,
                )

                # é€šçŸ¥ã‚’ä¿å­˜ï¼ˆassigneeã”ã¨ï¼‰
                for notif_create in notification_creates:
                    saved_notification = await notification_repo.create(notif_create)
                    notification_created = True
                    logger.info(f"ğŸ“¬ Task {task.id}: Created completion notification {saved_notification.id} with due_date {due_date}")
            except Exception as e:
                logger.error(f"Failed to create completion notification for task {task.id}: {e}")
        else:
            logger.warning(f"Task {task.id}: next_run_time is None, skipping notification creation")
        
        return {
            "status": "ok",
            "task_id": task.id,
            "task_title": task.title,
            "result_id": saved_result.id,
            "notification_created": notification_created
        }
        
    except Exception as e:
        logger.error(f"âŒ Error executing AI task {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to execute task: {str(e)}")