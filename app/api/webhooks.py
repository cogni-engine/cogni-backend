from fastapi import APIRouter
from app.config import supabase
from app.services.note_to_task import generate_tasks_from_note
from app.services.task_to_notification import generate_notifications_from_tasks_batch
from app.services.ai_task_executor import execute_ai_task
from app.services.ai_task_executor.completion_notification_service import generate_completion_notification
from app.infra.supabase.repositories.workspaces import WorkspaceRepository, WorkspaceMemberRepository
from app.infra.supabase.repositories.tasks import TaskRepository
from app.infra.supabase.repositories.notifications import AINotificationRepository
from app.infra.supabase.repositories.task_results import TaskResultRepository
from app.models.task import TaskUpdate, Task
from app.models.notification import AINotificationCreate
from app.models.task_result import TaskResultCreate
from app.utils.recurrence_calculator import calculate_next_run_time
import asyncio
from typing import List, Optional, Tuple
from datetime import datetime, timezone, timedelta
import logging

router = APIRouter(prefix="/api/webhooks", tags=["webhooks"])

# é–‹ç™ºè€…ã®user_idï¼ˆæœ¬ç•ªãƒ»localä¸¡æ–¹ã§åŒã˜ï¼‰
DEV_USER_IDS = [
    "58e744e7-ec0f-45e1-a63a-bc6ed71e10de",
]


async def _process_notes_sync(
    minutes_ago: int,
    user_id_filter: Optional[List[str]] = None,
    exclude_user_ids: bool = False
) -> dict:
    """
    ãƒãƒ¼ãƒˆåŒæœŸã®å…±é€šå‡¦ç†
    
    Args:
        minutes_ago: ä½•åˆ†å‰ã‹ã‚‰æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ã‹
        user_id_filter: æŒ‡å®šã•ã‚ŒãŸuser_idã®workspaceã®ãƒãƒ¼ãƒˆã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        exclude_user_ids: Trueã®å ´åˆã€user_id_filterã«å«ã¾ã‚Œã‚‹user_idã‚’é™¤å¤–
    
    Returns:
        å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    from datetime import datetime, timedelta, timezone
    from app.infra.supabase.repositories.notes import NoteRepository
    import logging
    
    logger = logging.getLogger(__name__)
    
    filter_desc = ""
    if user_id_filter:
        if exclude_user_ids:
            filter_desc = " (excluding dev users)"
        else:
            filter_desc = " (dev users only)"
    
    logger.info(f"ğŸ”„ CRON: Starting sync-memories{filter_desc}")
    
    # æŒ‡å®šæ™‚é–“å‰ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    time_ago = datetime.now(timezone.utc) - timedelta(minutes=minutes_ago)
    
    note_repo = NoteRepository(supabase)
    
    # æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒˆã®ã¿å–å¾—ï¼ˆuser_idãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ï¼‰
    updated_notes = await note_repo.find_updated_since(
        time_ago, 
        user_id_filter=user_id_filter,
        exclude_user_ids=exclude_user_ids
    )
    
    logger.info(f"Found {len(updated_notes)} updated notes{filter_desc}")
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    total_tasks_generated = 0
    total_notifications_generated = 0
    
    # ãƒãƒ¼ãƒˆå‡¦ç†é–¢æ•°
    async def process_note_with_limit(note):
        nonlocal total_tasks_generated, total_notifications_generated
        
        async with semaphore:
            try:
                # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                workspace_repo = WorkspaceRepository(supabase)
                workspace = await workspace_repo.find_by_id(note.workspace_id)
                
                if not workspace:
                    return {"status": "error", "note_id": note.id, "reason": "workspace_not_found"}
                
                if not note.text:
                    return {"status": "skipped", "note_id": note.id, "reason": "empty_text"}
                
                # workspace typeã«å¿œã˜ã¦(user_id, workspace_member_id)ãƒšã‚¢ã‚’å–å¾—
                user_workspace_member_pairs: List[Tuple[str, Optional[int]]] = []
                
                if workspace.type == "personal":
                    # personal workspaceã®å ´åˆ: ã‚ªãƒ¼ãƒŠãƒ¼ã®user_idã¨workspace_member_idã‚’å–å¾—
                    workspace_member_repo = WorkspaceMemberRepository(supabase)
                    members = await workspace_member_repo.find_by_workspace(note.workspace_id)
                    
                    if not members:
                        return {"status": "error", "note_id": note.id, "reason": "no_workspace_members"}
                    
                    user_workspace_member_pairs = [(members[0].user_id, members[0].id)]
                    
                elif workspace.type == "group":
                    # group workspaceã®å ´åˆ: assigneeã®(user_id, workspace_member_id)ãƒšã‚¢ã‚’å–å¾—
                    pairs = await note_repo.get_note_assignee_user_and_member_ids(note.id)
                    user_workspace_member_pairs = [(user_id, workspace_member_id) for user_id, workspace_member_id in pairs]
                    
                    if not user_workspace_member_pairs:
                        return {"status": "skipped", "note_id": note.id, "reason": "no_assignees"}
                
                # ãƒãƒ¼ãƒˆâ†’ã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼ˆ1å›ã®LLMå‘¼ã³å‡ºã—ã§å…¨ãƒšã‚¢åˆ†ã®ã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼‰
                tasks = await generate_tasks_from_note(note.id, note.text, user_workspace_member_pairs, note.title)
                tasks_count = len(tasks)
                total_tasks_generated += tasks_count
                
                # ã‚¿ã‚¹ã‚¯ãŒç”Ÿæˆã•ã‚ŒãŸã‚‰ã€å³åº§ã«é€šçŸ¥ã‚’ç”Ÿæˆ
                notifications_count = 0
                if tasks:
                    notifications = await generate_notifications_from_tasks_batch(tasks)
                    notifications_count = len(notifications)
                    total_notifications_generated += notifications_count
                    
                    logger.info(f"âœ… Note {note.id}: Generated {tasks_count} tasks and {notifications_count} notifications")
                else:
                    logger.info(f"âœ… Note {note.id}: No tasks generated")
                
                return {
                    "status": "ok",
                    "note_id": note.id,
                    "tasks_count": tasks_count,
                    "notifications_count": notifications_count
                }
                
            except Exception as e:
                logger.error(f"âŒ Error processing note {note.id}: {e}")
                return {"status": "error", "note_id": note.id, "error": str(e)}
    
    # ãƒãƒ¼ãƒˆã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_note_with_limit(note) for note in updated_notes],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(updated_notes)} notes processed{filter_desc}")
    logger.info(f"ğŸ“Š Generated {total_tasks_generated} tasks and ~{total_notifications_generated} notifications")
    
    return {
        "status": "ok",
        "notes_processed": success,
        "notes_total": len(updated_notes),
        "tasks_generated": total_tasks_generated,
        "notifications_generated": total_notifications_generated
    }


@router.post("/sync-memories")
async def sync_memories():
    """
    æœ¬ç•ªç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ5åˆ†ã”ã¨ï¼‰
    - 5åˆ†å‰ã‹ã‚‰ç¾åœ¨ã¾ã§ã«æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒˆã®ã¿ã‚’å‡¦ç†
    - é–‹ç™ºè€…ã®workspaceã‚’é™¤å¤–
    - ãƒãƒ¼ãƒˆâ†’ã‚¿ã‚¹ã‚¯ç”Ÿæˆâ†’é€šçŸ¥ç”Ÿæˆï¼ˆä¸€é€£ã®æµã‚Œã‚’å®Œçµï¼‰
    """
    return await _process_notes_sync(
        minutes_ago=5,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=True
    )


@router.post("/sync-memories-local")
async def sync_memories_local():
    """
    ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ1åˆ†ã”ã¨ï¼‰
    - 1åˆ†å‰ã‹ã‚‰ç¾åœ¨ã¾ã§ã«æ›´æ–°ã•ã‚ŒãŸãƒãƒ¼ãƒˆã®ã¿ã‚’å‡¦ç†
    - é–‹ç™ºè€…ã®workspaceã®ã¿ã‚’å‡¦ç†
    - ãƒãƒ¼ãƒˆâ†’ã‚¿ã‚¹ã‚¯ç”Ÿæˆâ†’é€šçŸ¥ç”Ÿæˆï¼ˆä¸€é€£ã®æµã‚Œã‚’å®Œçµï¼‰
    """
    # return await _process_notes_sync(
    #     minutes_ago=1,
    #     user_id_filter=DEV_USER_IDS,
    #     exclude_user_ids=False
    # )
    pass


@router.post("/process-recurring-tasks")
async def process_recurring_tasks():
    """
    å®šæœŸã‚¿ã‚¹ã‚¯ã®æ¬¡å›å®Ÿè¡Œæ™‚åˆ»ã‚’æ›´æ–°ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ1æ—¥1å›ã€åˆå‰0æ™‚ã«å®Ÿè¡Œï¼‰
    
    å‡¦ç†å†…å®¹:
    1. æ˜¨æ—¥ã®åˆå‰0æ™‚ã€œä»Šæ—¥ã®åˆå‰0æ™‚ã®ç¯„å›²ã§next_run_timeãŒè©²å½“ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    2. å„ã‚¿ã‚¹ã‚¯ã®next_run_timeã‚’æ¬¡å›å®Ÿè¡Œæ™‚åˆ»ã«æ›´æ–°
    3. is_recurring_task_active=Trueã®ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€æ—¢å­˜ã®é€šçŸ¥ã‚’è¤‡è£½ã—ã¦æ¬¡å›åˆ†ã‚’ä½œæˆ
    """
    logger = logging.getLogger(__name__)
    logger.info("ğŸ”„ CRON: Starting process-recurring-tasks")
    
    # æ˜¨æ—¥ã®åˆå‰0æ™‚ã¨ä»Šæ—¥ã®åˆå‰0æ™‚ã‚’å–å¾—
    now = datetime.now(timezone.utc)
    today_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)
    yesterday_midnight = today_midnight - timedelta(days=1)
    
    logger.info(f"Processing tasks with next_run_time between {yesterday_midnight} and {today_midnight}")
    
    # TaskRepositoryã§ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    task_repo = TaskRepository(supabase)
    notification_repo = AINotificationRepository(supabase)
    
    # next_run_timeãŒæ˜¨æ—¥ã®åˆå‰0æ™‚ã€œä»Šæ—¥ã®åˆå‰0æ™‚ã®ç¯„å›²ã«ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    # recurrence_patternãŒNULLã§ãªã„ã‚‚ã®ãŒå¯¾è±¡
    response = (
        supabase.table("tasks")
        .select("*")
        .gte("next_run_time", yesterday_midnight.isoformat())
        .lt("next_run_time", today_midnight.isoformat())
        .not_.is_("recurrence_pattern", "null")
        .execute()
    )
    
    recurring_tasks = response.data if response.data else []
    logger.info(f"Found {len(recurring_tasks)} recurring tasks to process")
    
    if not recurring_tasks:
        return {
            "status": "ok",
            "tasks_processed": 0,
            "notifications_created": 0
        }
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    tasks_updated = 0
    notifications_created = 0
    
    # ã‚¿ã‚¹ã‚¯å‡¦ç†é–¢æ•°
    async def process_task_with_limit(task_data):
        nonlocal tasks_updated, notifications_created
        
        async with semaphore:
            try:
                task_id = task_data["id"]
                old_next_run_time = datetime.fromisoformat(task_data["next_run_time"].replace("Z", "+00:00"))
                recurrence_pattern = task_data["recurrence_pattern"]
                is_active = task_data.get("is_recurring_task_active", True)
                
                # æ–°ã—ã„next_run_timeã‚’è¨ˆç®—
                new_next_run_time = calculate_next_run_time(old_next_run_time, recurrence_pattern)
                
                # ã‚¿ã‚¹ã‚¯ã®next_run_timeã‚’æ›´æ–°
                update_data = TaskUpdate(next_run_time=new_next_run_time)
                await task_repo.update(task_id, update_data)
                tasks_updated += 1
                
                logger.info(f"âœ… Task {task_id}: Updated next_run_time from {old_next_run_time} to {new_next_run_time}")
                
                # is_recurring_task_active=Trueã®å ´åˆã®ã¿ã€é€šçŸ¥ã‚’è¤‡è£½
                existing_notifications = []
                if is_active:
                    # æ—¢å­˜ã®é€šçŸ¥ã‚’å–å¾—
                    existing_notifications = await notification_repo.find_by_filters({"task_id": task_id})
                    
                    if existing_notifications:
                        # å„é€šçŸ¥ã‚’è¤‡è£½
                        for notification in existing_notifications:
                            # å…ƒã®due_dateã¨old_next_run_timeã®å·®åˆ†ã‚’è¨ˆç®—
                            time_diff = notification.due_date - old_next_run_time
                            
                            # æ–°ã—ã„due_dateã‚’è¨ˆç®—
                            new_due_date = new_next_run_time + time_diff
                            
                            # æ–°ã—ã„é€šçŸ¥ã‚’ä½œæˆ
                            new_notification = AINotificationCreate(
                                title=notification.title,
                                ai_context=notification.ai_context,
                                body=notification.body,
                                due_date=new_due_date,
                                task_id=task_id,
                                user_id=notification.user_id,
                                workspace_member_id=notification.workspace_member_id,
                                status=notification.status
                            )
                            
                            await notification_repo.create(new_notification)
                            notifications_created += 1
                            
                            logger.info(f"ğŸ“¬ Task {task_id}: Created notification with due_date {new_due_date}")
                    else:
                        logger.info(f"â„¹ï¸ Task {task_id}: No existing notifications to duplicate")
                else:
                    logger.info(f"â„¹ï¸ Task {task_id}: is_recurring_task_active=False, skipping notification duplication")
                
                return {
                    "status": "ok",
                    "task_id": task_id,
                    "notifications_created": len(existing_notifications) if is_active else 0
                }
                
            except Exception as e:
                logger.error(f"âŒ Error processing task {task_data.get('id')}: {e}")
                return {"status": "error", "task_id": task_data.get("id"), "error": str(e)}
    
    # ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_task_with_limit(task) for task in recurring_tasks],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(recurring_tasks)} tasks processed")
    logger.info(f"ğŸ“Š Tasks updated: {tasks_updated}, Notifications created: {notifications_created}")
    
    return {
        "status": "ok",
        "tasks_processed": tasks_updated,
        "notifications_created": notifications_created
    }


async def _execute_ai_tasks_common(
    minutes_ahead: int,
    user_id_filter: Optional[List[str]] = None,
    exclude_user_ids: bool = False
) -> dict:
    """
    AIã‚¿ã‚¹ã‚¯è‡ªå‹•å®Ÿè¡Œã®å…±é€šå‡¦ç†
    
    Args:
        minutes_ahead: ä½•åˆ†å…ˆã¾ã§ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‹
        user_id_filter: æŒ‡å®šã•ã‚ŒãŸuser_idã®ã‚¿ã‚¹ã‚¯ã®ã¿å‡¦ç†ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
        exclude_user_ids: Trueã®å ´åˆã€user_id_filterã«å«ã¾ã‚Œã‚‹user_idã‚’é™¤å¤–
    
    Returns:
        å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
    """
    logger = logging.getLogger(__name__)
    
    filter_desc = ""
    if user_id_filter:
        if exclude_user_ids:
            filter_desc = " (excluding dev users)"
        else:
            filter_desc = " (dev users only)"
    
    logger.info(f"ğŸ”„ CRON: Starting execute-ai-tasks{filter_desc}")
    
    # ç¾åœ¨æ™‚åˆ»ã¨æŒ‡å®šåˆ†å¾Œã®æ™‚åˆ»ã‚’å–å¾—
    now = datetime.now(timezone.utc)
    target_time = now + timedelta(minutes=minutes_ahead)
    
    logger.info(f"Processing AI tasks with next_run_time or deadline between {now} and {target_time}{filter_desc}")
    
    # TaskResultRepositoryã¨AINotificationRepositoryã‚’åˆæœŸåŒ–
    task_result_repo = TaskResultRepository(supabase)
    notification_repo = AINotificationRepository(supabase)
    
    # æ¡ä»¶ã«åˆã†ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
    # is_recurring_task_active=True AND is_ai_task=True AND
    # (now < next_run_time <= target_time OR now < deadline <= target_time)
    try:
        response = (
            supabase.table("tasks")
            .select("*")
            .eq("is_recurring_task_active", True)
            .eq("is_ai_task", True)
            .or_(
                f"and(next_run_time.gt.{now.isoformat()},next_run_time.lte.{target_time.isoformat()}),"
                f"and(deadline.gt.{now.isoformat()},deadline.lte.{target_time.isoformat()})"
            )
            .execute()
        )
        
        ai_tasks = response.data if response.data else []
        logger.info(f"Found {len(ai_tasks)} AI tasks to execute{filter_desc}")
        
    except Exception as e:
        logger.error(f"Failed to query AI tasks: {e}")
        return {
            "status": "error",
            "error": str(e),
            "tasks_executed": 0
        }
    
    # user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
    if user_id_filter and ai_tasks:
        if exclude_user_ids:
            # é–‹ç™ºè€…ã‚’é™¤å¤–
            ai_tasks = [task for task in ai_tasks if task.get("user_id") not in user_id_filter]
            logger.info(f"After excluding dev users: {len(ai_tasks)} AI tasks")
        else:
            # é–‹ç™ºè€…ã®ã¿
            ai_tasks = [task for task in ai_tasks if task.get("user_id") in user_id_filter]
            logger.info(f"After filtering dev users only: {len(ai_tasks)} AI tasks")
    
    if not ai_tasks:
        return {
            "status": "ok",
            "tasks_executed": 0,
            "results_saved": 0
        }
    
    # ã‚»ãƒãƒ•ã‚©ã§ä¸¦åˆ—å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼ˆ10ä¸¦åˆ—ï¼‰
    semaphore = asyncio.Semaphore(10)
    
    # çµ±è¨ˆæƒ…å ±
    tasks_executed = 0
    results_saved = 0
    notifications_created = 0
    
    # ã‚¿ã‚¹ã‚¯å‡¦ç†é–¢æ•°
    async def process_ai_task_with_limit(task_data):
        nonlocal tasks_executed, results_saved, notifications_created
        
        async with semaphore:
            try:
                # Taskãƒ‡ãƒ¼ã‚¿ã‚’Taskãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
                task = Task(**task_data)
                
                logger.info(f"ğŸ¤– Executing AI task {task.id}: {task.title}")
                
                # AIã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆtitleã¨textã®ä¸¡æ–¹ã‚’å–å¾—ï¼‰
                result_title, result_text = await execute_ai_task(task)
                tasks_executed += 1
                
                # çµæœã‚’task_resultsã«ä¿å­˜
                task_result_create = TaskResultCreate(
                    task_id=task.id,
                    result_title=result_title,
                    result_text=result_text,
                    executed_at=datetime.now(timezone.utc)
                )
                
                saved_result = await task_result_repo.create(task_result_create)
                results_saved += 1
                
                logger.info(f"âœ… Task {task.id} executed and saved: result_id={saved_result.id}")
                
                # å®Œäº†é€šçŸ¥ã‚’ç”Ÿæˆ
                # due_dateã¯next_run_timeãŒã‚ã‚Œã°ãã‚Œã€ãªã‘ã‚Œã°deadlineã‚’ä½¿ç”¨
                due_date = task.next_run_time if task.next_run_time else task.deadline
                
                if due_date:
                    try:
                        notification = await generate_completion_notification(
                            task=task,
                            result_title=result_title,
                            result_text=result_text,
                            due_date=due_date
                        )
                        
                        # é€šçŸ¥ã‚’ä¿å­˜
                        saved_notification = await notification_repo.create(notification)
                        notifications_created += 1
                        
                        logger.info(f"ğŸ“¬ Task {task.id}: Created completion notification {saved_notification.id} with due_date {due_date}")
                    except Exception as e:
                        logger.error(f"Failed to create completion notification for task {task.id}: {e}")
                else:
                    logger.warning(f"Task {task.id}: No next_run_time or deadline, skipping notification creation")
                
                return {
                    "status": "ok",
                    "task_id": task.id,
                    "result_id": saved_result.id
                }
                
            except Exception as e:
                logger.error(f"âŒ Error executing AI task {task_data.get('id')}: {e}")
                return {"status": "error", "task_id": task_data.get("id"), "error": str(e)}
    
    # ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
    results = await asyncio.gather(
        *[process_ai_task_with_limit(task) for task in ai_tasks],
        return_exceptions=True
    )
    
    # çµæœé›†è¨ˆ
    success = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ok")
    
    logger.info(f"ğŸ‰ CRON completed: {success}/{len(ai_tasks)} AI tasks executed{filter_desc}")
    logger.info(f"ğŸ“Š Tasks executed: {tasks_executed}, Results saved: {results_saved}, Notifications created: {notifications_created}")
    
    return {
        "status": "ok",
        "tasks_executed": tasks_executed,
        "results_saved": results_saved,
        "notifications_created": notifications_created
    }


@router.post("/execute-ai-tasks")
async def execute_ai_tasks():
    """
    æœ¬ç•ªç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ10åˆ†ã”ã¨ï¼‰
    - æ¬¡ã®10åˆ†ä»¥å†…ã«next_run_timeã¾ãŸã¯deadlineãŒæ¥ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    - é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
    """
    return await _execute_ai_tasks_common(
        minutes_ahead=10,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=True
    )


@router.post("/execute-ai-tasks-local")
async def execute_ai_tasks_local():
    """
    ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨CRONå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ1åˆ†ã”ã¨ï¼‰
    - æ¬¡ã®1åˆ†ä»¥å†…ã«next_run_timeã¾ãŸã¯deadlineãŒæ¥ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    - é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’å‡¦ç†
    """
    return await _execute_ai_tasks_common(
        minutes_ahead=1,
        user_id_filter=DEV_USER_IDS,
        exclude_user_ids=False
    )